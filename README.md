# BeyondChats

The project is implemented in three phases as described in the assignment and demonstrates backend development, web scraping, AI-based content enhancement, and frontend UI development.


## ğŸ“Œ Project Overview

The goal of this project is to:

Scrape the oldest blog articles from BeyondChats .

Store and manage them via CRUD APIs.

Enhance article content using AI (LLM) with reference articles.

Display both original and AI-updated articles in a responsive React frontend.


## ğŸ§© Project Phases
### âœ… Phase 1: Web Scraping & Backend APIs

Scraped the 5 oldest articles from: 
https://beyondchats.com/blogs/

#### Extracted: 
Title 
URL 
Date 
Stored articles in MongoDB 
Built CRUD APIs using Node.js, Express, and Mongoose 


### âœ… Phase 2: AI-based Article Enhancement

#### Created a Node.js script (manual execution) that:

Fetches stored articles from the database 
Searches related articles (mocked Google search) 
Uses Gemini LLM to rewrite and enhance content 
Preserves original meaning while improving structure and clarity 
Appends reference links at the bottom 
Updated articles are saved back to MongoDB with an isUpdated flag 

âš ï¸ This script is intentionally not deployed and is run manually to avoid serverless execution limits.



### âœ… Phase 3: Frontend (React)

Built a React (Vite) frontend that:

Fetches articles from backend APIs

Displays articles in a responsive card-based UI

Shows:

Title

Date

Updated badge

Content

Reference links

## ğŸ—ï¸ Architecture & Data Flow
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend   â”‚  (React + Vite)
â”‚            â”‚
â”‚  Fetches   â”‚
â”‚  Articles  â”‚
â””â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜
      â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
â”‚ Backend    â”‚  (Node.js + Express)
â”‚            â”‚
â”‚ CRUD APIs  â”‚
â”‚ Scraper    â”‚
â”‚ AI Script  â”‚
â””â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜
      â”‚
â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
â”‚ MongoDB    â”‚
â”‚ (Atlas)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



## Important principle:
ğŸ‘‰ The frontend never talks to the database directly.
ğŸ‘‰ All data access happens through backend APIs.



## âš™ï¸ Tech Stack
### Backend

Node.js

Express.js

MongoDB (Atlas)

Mongoose

Axios

Cheerio

Gemini LLM API


### Frontend

React (Vite)

JavaScript

CSS (custom, responsive)

ğŸš€ Local Setup Instructions
1ï¸âƒ£ Clone the repository
git clone <your-github-repo-url>
cd BeyondChats

## 2ï¸âƒ£ Backend Setup
cd backend 
npm install 
Create a .env file inside backend/: 
PORT=5000 
MONGO_URI=your_mongodb_connection_string 
GEMINI_API_KEY=your_gemini_api_key 

Run backend server: 
npm run dev 
Backend will run at: 
http://localhost:5000 


### 3ï¸âƒ£ Run Scraper (Phase 1)
node scrapeOldestBlogs.js

This will:

Scrape oldest articles

Save them to MongoDB

### 4ï¸âƒ£ Run AI Update Script (Phase 2)
node updateArticles.js

This will:

Enhance article content using AI

Save updated versions to DB


### 5ï¸âƒ£ Frontend Setup
cd ../frontend
npm install
npm run dev

Frontend runs at:

http://localhost:5173

### ğŸŒ Live Links

Frontend (Vercel):
ğŸ‘‰ To be added

Backend (Render):
ğŸ‘‰ To be added



### ğŸ“Š Evaluation Criteria Alignment

This project satisfies all evaluation criteria mentioned in the assignment:

âœ… Completeness

âœ… Clean & structured README

âœ… Responsive UI

âœ… Clear backendâ€“frontend separation

âœ… Scalable architecture



### ğŸ“ Notes

The AI update script is executed manually, not via frontend or deployment.

Mock search is used in place of Google Search API to avoid paid services.

Reference URLs are still cited at the bottom of updated articles.



### ğŸ™Œ Final Note

This project demonstrates:

Real-world backend practices

AI integration

Data flow understanding

Clean frontend architecture

Thank you for the opportunity!
